**第三章**

**1、章节主要内容**

线性模型形式简单、易于建模，许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得（神经网络就是一个例子，大伙在后边学习神经网络时会发现其实神经网络的每个神经元就是一个广义的线性模型）

本章内容对几种经典的线性模型进行了介绍，并讨论其在二分类和多分类任务上的应用以及需考虑的问题。

**1）线性模型的背后逻辑以及变型思路**

线性模型的本质是通过训练数据学习出一个通过样本数据的属性的线性组合来进行预测的函数，其基本格式可表现如下：

f(x) = w1x1 + w2x2 + ... + wnxn + b   (公式1)

其中xi是样本的第i个属性的属性值，wi是线性模型对第i个属性的权重，b是模型的线性偏移量。因为w直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性。

**［1］线性模型最基础的算法：线性回归**

线性回归就是使用训练样本(x,y)去训练上边的(公式1)，使得生成的函数f(x)能尽量贴近样本标签y。

因为是函数训练，所以我们在训练之前需要将离散值属性转换为连续值属性。并且该属性的属性值之间是否存在“序”关系，会有不同的转换方式。如果是有序属性，我们会根据属性序数高低给予一个数字序列，比如身高，具有“高、中等、矮”三个有序属性值，那么我们就能将其转换为{1,0.5,0}；如果属性值之间没有序列，我们要将这k个属性值转为k维向量，比如天气，“下雨、晴天、多云”三个属性值之间并没有序列关系，那么就应该将其转换为3维向量，(1,0,0)，(0,1,0)，(0,0,1)

（上边的属性转换过程虽然简单，但是要获得好的学习效果，不能不注重）

在对数据进行转换之后，为了能生成线性回归模型，我们需要计算出正确的w和b。回顾我们线性回归的函数目的，我们发现线性回归是为了能使得生成的回归函数能更贴近样本标签，也就是说当训练出来的模型f(x)和真正的值y之间的误差最小时，该线性回归模型就是我们要的。

这就转换为了求最小值的问题了，线性回归函数是使用均方误差和最小二乘法来进行计算的。其背后的逻辑是通过计算训练样本在向量空间上距离模型线的欧式距离之和的最小值来确定参数w和b。而这个最小值可以通过对距离之和求导获得，当导数为0时，说明原函数达到了一个极值（因为是凸函数，所以该极值是最小值）。

**［2］线性回归函数的变型**

线性回归函数可以简写为：y = wx + b，这是用模型的预测值去逼近真实标记y。那么我们能不能令预测值去逼近y的衍生物呢？基于这样的考虑，我们就得到了线性回归的各种变型函数。

比如，假设我们认为示例所对应的输出标记是在指数尺度上变化的，那么就可将输出标记的对数作为线性模型逼近的目标，即可得到“对数线性回归”（log-linear regression）：

ln(y) = wx + b

更一般地，考虑单调可微函数g，令：

g(y) = wx + b

这样得到的模型称为“广义线性模型”（generalized linear model），其中函数g称为“联系函数”（link function）。不同的联系函数会构成不同的线性回归模型。

其中，当 g(y) = ln( y / (1-y) ) 时，我们得到了“对数几率回归”（logit regression），也称“逻辑回归函数”。通过联系函数，我们可以看出该模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。

**2）线性模型在多分类场景下的应用**

在现实中常遇到多分类学习任务，我们在多数情况下，将基于一些基本策略，利用二分类学习器来解决多分类问题。

多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解。

最经典的拆分策略有以下三种：

**［1］“一对一”（One vs One，简称OvO）**

将N个类别两两配对，对每一种组合训练一个分类器，从而产生N(N-1)/2个二分类任务。当输入新样本时，用N(N-1)/2个学习器分别进行分类，并对分类结果进行计数，被预测最多的类为最终分类结果。

线性判别分析

线性判别分析（ Linear Discriminat Analysis，LDA），是一种经典的线性学习方法。(由于由Fisher首次提出，又称Fisher线性判别分析)　　

其朴素思想为：　给定训练集，将样例投影到一条直线上，是的同类的投影点尽可能近，不同类的投影点尽可能远；对新样本做分类时，同样将其投影到这条直线上，在根据投影点的位置